<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Parallel Programming with CUDA &#8211; Trailblazer</title>
<meta name="description" content="">
<meta name="keywords" content="gpu,CUDA">


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Parallel Programming with CUDA">
<meta property="og:description" content="The personal webpage and weblog of Avi Singh.">
<meta property="og:url" content="http://avisingh599.github.io/gpu/parallel-programming-with-cuda/">
<meta property="og:site_name" content="Trailblazer">





<link rel="canonical" href="http://avisingh599.github.io/gpu/parallel-programming-with-cuda/">
<link href="http://avisingh599.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Trailblazer Feed">
<link rel="author" href="http://plus.google.com/+avisingh599?rel=author">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://avisingh599.github.io/assets/css/main.min.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://avisingh599.github.io/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://avisingh599.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://avisingh599.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://avisingh599.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://avisingh599.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://avisingh599.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://avisingh599.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://avisingh599.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://avisingh599.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-63278661-1', 'auto');
  ga('send', 'pageview');

</script>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://avisingh599.github.io">Trailblazer</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://avisingh599.github.io/" >Home</a></li>
		        
				<li><a href="http://avisingh599.github.io/about/" >About</a></li>
		        
				<li><a href="http://avisingh599.github.io/assets/cv.pdf" >Resume</a></li>
		        
				<li><a href="http://avisingh599.github.io/research/" >Research</a></li>
		        
				<li><a href="http://avisingh599.github.io/projects/" >Projects</a></li>
		        
				<li><a href="http://avisingh599.github.io/acads/" >Academics</a></li>
		        
				<li><a href="http://avisingh599.github.io/achievements/" >Achievements</a></li>
		        
				<li><a href="http://avisingh599.github.io/posts/" >Posts</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://avisingh599.github.io/images/bio-photo.jpg" class="bio-photo" alt="Avi Singh bio photo"></a>

<h3>Avi Singh</h3>
<p>Undergraduate, interested in Computer Vision</p>

<a href="http://facebook.com/avisingh94" class="author-social" target="_blank"><i class="fa fa-facebook-square"></i> Facebook</a>
<a href="http://plus.google.com/+avisingh599" class="author-social" target="_blank"><i class="fa fa-google-plus-square"></i> Google+</a>



<a href="http://github.com/avisingh599" class="author-social" target="_blank"><i class="fa fa-github"></i> Github</a>






  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://avisingh599.github.io/gpu/parallel-programming-with-cuda/" rel="bookmark" title="Parallel Programming with CUDA">Parallel Programming with CUDA</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <p>I recently started going through an amazing Udacity course on Parallel Programming. Having been working on image processing and computer vision for quite some time now, I have realized that CPUs are NOT designed for image processing applications. Even the oh-so-optimized OpenCV implementations of computer vision algorithms in C/C++ do not give a good speed when working on something as computationally expensive as variational optical flow. However, if you use the inbuilt CUDA module (in OpenCV 3.0), the performance is <em>way</em> better.</p>

<h2 id="why-gpus">Why GPUs?</h2>

<p>CPUs are not getting any faster, due to limitation of clock speeds which have virtually remained the same since the past 5 years or so. Increasing this clock speed has become close to impossible, since increasing clock speeds increases the power consumption, which makes it difficult to cool the CPU. So, for faster computations, GPUs are the way to go.</p>

<h2 id="gpu-vs-cpu">GPU vs CPU</h2>

<p>My computer has a quad-core processor with hyper threading (an Intel i7 Ivy Bridge). This means that, in the best case, I can have at most 8-threads truly running in parallel. On the other hand, the low-end GPU that I have (nVidia GeForce GT630M) has 96 cores!</p>

<p>In general, a CPU has a few, very powerful computation cores, where as a GPU has a very large number of smaller, less powerful computation cores. The time taken to perform any one particular task is less on the CPU, but if you need to performs thousands of such tasks, then the GPU would beat the CPU.</p>

<p>One more important thing to note is that, while designing CPUs engineers optimize for <em>latency</em>. On the other hand, maximum <em>thorughput</em> is what the designers are aiming at while making GPUs.</p>

<h2 id="throughput-vs-latency">Throughput vs Latency</h2>

<ul>
  <li>
    <p>Throughput: It is defined as the amount of work done in unit time. For example, I need to transport 50 bags of rice from ground floor of a building to the 10th floor, and I can carry at most two bags at a time. Let the time taken in each trip be 5 minutes. So, the amount of work done in one hour would be 2*(60/5) = 24 bags. I can say that the throughput is 24 bags/hr.</p>
  </li>
  <li>
    <p>Latency: It is defined as the amount of time taken to perform a particular task. In the previous example, it would take take 125 minutes to take all the 50 bags, and hence the latency (measures in time units) is 125 minutes.</p>
  </li>
</ul>

<h2 id="cuda">CUDA</h2>

<p>CUDA is a framework developed by nVidia for writing programs that run both on the GPU and the CPU. On the CPU side, you can write programs in C, and then used some extensions to C (written by nVidia) to write programs that run on the GPU. These programs that run on the GPU are called <em>kernels</em>. A kernel looks like a serial program, but the CPU launches on a large  number of threads on the GPU. In CUDA, the CPU is referred to as the <em>host</em> while the GPU is referred to as the <em>device</em>. In this relationship between the CPU and the GPU, the CPU is the <em>alpha</em>. The CPU and the GPU have separate memories, and can perform operations only on the data that is stored in their own memory. The CPU can allocate memory on the GPU, copy data from the CPU memory to the GPU memory, launch kernels on hundreds of thread on the GPU, and copy back the results from the GPU memory. The GPU, on the other hand, can only respond to call of memory copy made by the CPU, and cannot make its own requests for data transfer.</p>

<h4 id="skeleton-of-a-cuda-program">Skeleton of a CUDA program:</h4>

<ul>
  <li>Allocate memory on the GPU</li>
  <li>Transfer data from the CPU memory to the GPU memory</li>
  <li>Perform the computations on the GPU</li>
  <li>Copy the results from the GPU to the CPU</li>
</ul>

<p>A sample code in CUDA, which calculates the cubes of all integers from 1 to 64.</p>

<div class="highlight"><pre><code class="c"><span class="cp">#include &lt;stdio.h&gt;</span>

<span class="c1">// here is the kernel</span>

<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">cube</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span> <span class="n">d_out</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span> <span class="n">d_in</span><span class="p">){</span>
	<span class="c1">// Todo: Fill in this function</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">f</span> <span class="o">=</span> <span class="n">d_in</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
    <span class="n">d_out</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="o">*</span><span class="n">f</span><span class="o">*</span><span class="n">f</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// threadIdx is a C struct having members x,y,z, other structs available are blockIdx, threaddim, blockdim</span>
<span class="c1">//__global__ is what specifies that the fucntion is a kernel</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">const</span> <span class="kt">int</span> <span class="n">ARRAY_SIZE</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
	<span class="k">const</span> <span class="kt">int</span> <span class="n">ARRAY_BYTES</span> <span class="o">=</span> <span class="n">ARRAY_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>

	<span class="c1">// generate the input array on the host</span>
	<span class="kt">float</span> <span class="n">h_in</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">];</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">h_in</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kt">float</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="kt">float</span> <span class="n">h_out</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">];</span>

	<span class="c1">// declare GPU memory pointers</span>
	<span class="kt">float</span> <span class="o">*</span> <span class="n">d_in</span><span class="p">;</span>
	<span class="kt">float</span> <span class="o">*</span> <span class="n">d_out</span><span class="p">;</span>

	<span class="c1">// allocate GPU memory</span>
	<span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">d_in</span><span class="p">,</span> <span class="n">ARRAY_BYTES</span><span class="p">);</span>
	<span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">d_out</span><span class="p">,</span> <span class="n">ARRAY_BYTES</span><span class="p">);</span>

	<span class="c1">// transfer the array to the GPU</span>
	<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">ARRAY_BYTES</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

	<span class="c1">// launch the kernel</span>
	<span class="n">cube</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_out</span><span class="p">,</span> <span class="n">d_in</span><span class="p">);</span>
	<span class="cm">/*</span>
<span class="cm">	One block of 64 threads is being launched here. We specify the number of blocks as well as the number of threads in each block.</span>
<span class="cm">	Each block has a limited number of threads that it can support. Modern GPUs support 1024, older support 512.</span>
<span class="cm">	Can have any number of blocks. Cuda supports 2D and 3D arrangement of blocks as well.</span>
<span class="cm">	*/</span>

	<span class="c1">// copy back the result array to the CPU</span>
	<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">ARRAY_BYTES</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

	<span class="c1">// print out the resulting array</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">&quot;%f&quot;</span><span class="p">,</span> <span class="n">h_out</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
		<span class="n">printf</span><span class="p">(((</span><span class="n">i</span> <span class="o">%</span> <span class="mi">4</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">?</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">&quot;</span> <span class="o">:</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
	<span class="p">}</span>

	<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_in</span><span class="p">);</span>
	<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_out</span><span class="p">);</span>

	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></div>

<p>If you have the nVidia CUDA toolkit installed, you can compile and run the above program using:</p>

<div class="highlight"><pre><code class="bash">nvcc -o sample sample.c
./sample</code></pre></div>


      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<img src="http://avisingh599.github.io/images/bio-photo.jpg" class="bio-photo" alt="Avi Singh bio photo"></a>

<h3>Avi Singh</h3>
<p>Undergraduate, interested in Computer Vision</p>

<a href="http://facebook.com/avisingh94" class="author-social" target="_blank"><i class="fa fa-facebook-square"></i> Facebook</a>
<a href="http://plus.google.com/+avisingh599" class="author-social" target="_blank"><i class="fa fa-google-plus-square"></i> Google+</a>



<a href="http://github.com/avisingh599" class="author-social" target="_blank"><i class="fa fa-github"></i> Github</a>






        </div>
        <p class="byline"><strong>Parallel Programming with CUDA</strong> was published on <time datetime="2014-07-21T00:00:00+00:00">July 21, 2014</time> and last modified on <time datetime="2014-07-20">July 20, 2014</time> by <a href="http://avisingh599.github.io/about" title="About Avi Singh">Avi Singh</a>.</p>
      </footer>
    </div><!-- /.article-wrap -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="http://avisingh599.github.io/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="http://avisingh599.github.io/machinelearning/classifying-human-activites-kinect/" title="Recognizing Human Activities with Kinect - Choosing a temporal model">Recognizing Human Activities with Kinect - Choosing a temporal model</a></li>
    
      <li><a href="http://avisingh599.github.io/vision/visual-odometry-full/" title="Visual Odmetry from scratch - A tutorial for beginners">Visual Odmetry from scratch - A tutorial for beginners</a></li>
    
      <li><a href="http://avisingh599.github.io/vision/stichting-story/" title="Stitching Intra-Oral Images">Stitching Intra-Oral Images</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  <footer>
    <span>&copy; 2015 Avi Singh. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://avisingh599.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://avisingh599.github.io/assets/js/scripts.min.js"></script>

	        

</body>
</html>